{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "6_b_AutoSkLearn_Regression_NY_Taxy.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twMqII6SXM7f",
        "colab_type": "text"
      },
      "source": [
        "# Block 6 Exercise 2: finding the best parameters for predicting the fare of taxi rides\n",
        "We return to our Random Forest Regression and want to automatically optimize all free parameters ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLPIq0N9XM7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import folium\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAlV0Dy4XM7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1484f323-72f8-4869-e566-af574dc2b84d"
      },
      "source": [
        "#check if notebook runs in colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print('running in Colab:',IN_COLAB)\n",
        "path='..'\n",
        "if IN_COLAB:\n",
        "  #in colab, we need to clone the data from the repo\n",
        "  !git clone https://github.com/keuperj/DataScienceSS20.git\n",
        "  path='DataScienceSS20'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running in Colab: True\n",
            "fatal: destination path 'DataScienceSS20' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqHNsMlWXM7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we load the data we have saved after wrangling and pre-processing in block I\n",
        "X=pd.read_csv(path+'/DATA/train_cleaned.csv')\n",
        "drop_columns=['Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','key','pickup_datetime','pickup_date','pickup_latitude_round3','pickup_longitude_round3','dropoff_latitude_round3','dropoff_longitude_round3']\n",
        "X=X.drop(drop_columns,axis=1)\n",
        "X=pd.get_dummies(X)# one hot coding\n",
        "#generate labels\n",
        "y=X['fare_amount']\n",
        "X=X.drop(['fare_amount'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvNDKlUAXM7q",
        "colab_type": "text"
      },
      "source": [
        "### Scikit Optimize\n",
        "Scikit Optimize (https://scikit-optimize.github.io/stable/index.html) is a AutoML toolbox wrapped around Scikit-Learn. It allows us to use state-of-the-art automatic hyper-parameter optimization on top of our learning algorithms.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFEuASn-XM7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8473d68f-4601-4ac9-a0f5-a037a25946ec"
      },
      "source": [
        "# install \n",
        "!pip install scikit-optimize"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.18.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.15.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QrrMaVFXM7t",
        "colab_type": "text"
      },
      "source": [
        "### E 2.1 Bayesian Optimization of a Random Forest Regression Model\n",
        "use Bayesian Optimization with Cross-Validation (https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV) to find the best regression model. Compare\n",
        "* linear regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) \n",
        "* Random Forest regression (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
        "* and SVM regression (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)\n",
        "\n",
        "NOTES: this can become quite compute intensive! Hence,\n",
        "* use a smaller subset of the training data to run the experiments \n",
        "* think about the range of your parameters (e.g. larger number of trees in RF or high C-values in SMV will make models expensive)\n",
        "* optimize only the following parameters per model type:\n",
        "    * linear: no parameters to optimize\n",
        "    * RF: #trees and depth\n",
        "    * SVM: C and gamma (use RBF kernel)\n",
        "* parallelize -> n_jobs\n",
        "* use CoLab to rum the job for up to 12h \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7yqIsiBXM7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foXkqomKX85a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "split_train_X = train_X.to_numpy()[:1000, :]\n",
        "split_train_Y = train_Y.to_numpy()[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siUT1iIyYlkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svr_param = dict(C=[0.1,1,2,5,10],gamma=[10,5, 1,0.1,'scale'])\n",
        "rf_param = dict(n_estimators=[10,20,30], max_depth=[1,2,3,4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hi6C2w_YbiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svr_model = SVR()\n",
        "linear_model = LinearRegression()\n",
        "rf_model = RandomForestRegressor()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW_NnlP1cUNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [linear_model, svr_model, rf_model]\n",
        "params = [{}, svr_param, rf_param]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ZN2EAeadBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_size = len(models)\n",
        "\n",
        "for i in range(model_size):\n",
        "    opt = BayesSearchCV(models[i], params[i], n_jobs=4, n_iter=32, random_state=0)\n",
        "    opt.fit(split_train_X, split_train_Y)\n",
        "\n",
        "    print(\"score:\", opt.score(test_X, test_Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NmX99wfd1Sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "921418d9-2c65-4dac-bcf9-c7cbb892c329"
      },
      "source": [
        "Integer(2,20)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Integer(low=2, high=20, prior='uniform', transform='identity')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}